---
layout: post
comments: true
title:  "Recurrent Neural Networks - 01 General Information"
excerpt: "Wiebke GÃ¼nther - This series of blogposts will cover a small portion of what 
there is to know about Recurrent Neural Networks (RNNs). 
Within this very wide topic it will focus on the special 
cases of Long Short-Term Memory (LSTM) and Neural Turing 
Machines."
date:   2017-09-14
mathjax: true
---

This series of blogposts will cover a small portion of what 
there is to know about Recurrent Neural Networks (RNNs). 
Within this very wide topic it will focus on the special 
cases of Long Short-Term Memory (LSTM) and Neural Turing 
Machines. This first part of a series of blogposts will focus 
on the general concept of RNNs and the problems arising when 
training them.
I will assume that the reader is familiar with the concept of 
feedforward neural networks.

## Introduction
Speech, text, music. Many aspects of our daily life, things that 
are natural and common to us, happen in sequential form. 
They have an underlying structure, rules that hold true for 
every part of the sequence (for every word or even letter, for 
every note or sound). For example, it would be an utter waste 
of time and mental capacity to try to learn the rules of a 
language at every position in the sequence separately. 
Surely, it would be better to use previous knowledge and not 
start thinking from scratch at every second.


With that in mind, it's a shame that feedforward neural networks 
are so restricted in what they accept as input. They can only 
operate on fixed-sized vectors and have separate parameters 
for each input feature. This means those networks can't 
operate on arbitrarily long sequences of vectors and they 
have no way of using knowledge of previous parts of a sequence 
to inform later ones. They don't posses the power of memory.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/simplest_MLP_vector.png)

(Here we have a very easy representation of a feedforward 
network with input and output vector and the green vector 
holds the hidden states.)


So, is there any way to solve this problem? 

Indeed, there is 
and it is called recurrence. It basically means that we add 
loops to the network. This allows for parameter sharing across 
several time steps.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/with_loop.png)

(Now we have added a loop to our network from above. 
The black square indicates a delay of a single time step.)


## Computational Graph and Unfolding

To make things a bit easier we will consider a network without 
any output in the next paragraph.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/with_loop_without_output.png)

Now we need to understand the meaning of those loops. The 
looping arrow in the picture shows us that each member of 
the output is a function of the previous members of the 
output. In mathematical notation:

$$ h_{t} = \phi(h_{t-1}, x_{t}; \theta), \qquad t=0, \ldots, \tau $$

where $ h_{t} $ is the state of the network at time $ t $, $ h_{0} $ 
being specified, $ \phi $ is a function, $ x_t $ is an 
external signal and $ \theta $ are some parameters.

We can also rewrite this expression in its non-recurrent form, 
e.g. for $\tau=2$:
$$ h_{2} = \phi(h_{1}, x_{2}; \theta) = \phi(\phi(h_{0}, x_{1}; \theta), x_{2}; \theta) = g_{2}(x_{2}, x_{1})$$

Or for general $\tau \in \mathbb{N}$:
$$h_{\tau} = \phi(h_{\tau-1}, x_{\tau}; \theta) = \phi(\phi(h_{\tau-2}, x_{\tau-1}; \theta), x_{\tau}; \theta) = \ldots = \phi(\ldots(\phi(h_{0}, x_{1}; \theta)\ldots) x_{\tau}; \theta) = g_{\tau}(x_{T}, \ldots, x_{1})$$

This gives us the idea for a different, more explicit, way 
of drawing our RNN:

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/unfolded_without_output.png)

(The unfolded graph. This is the same network as above, 
just drawn in a different way.)

Drawing the network in this ways shows us the power of RNNs 
quite explicitly:
We see that it is possible to learn a single model $\phi$ 
(the shared  model) which operates on all time steps and all 
sequence length rather than having to learn a separate model 
$g_{t}$ for all possible time steps! This learned model then 
has, regardless of sequence size, always the same input size 
and it uses the same parameters in every time step.

This is remarkable since it allows for generalization to 
sequence lengths that did not appear in the training set.

## Design Patterns and Implementation
There are many different ways to design a RNN. For instance, 
you could have RNNs with outputs at each time step and 
recurrent connections between hidden units, or RNNs with 
outputs at each time step and recurrent connections only 
from the output at one time step to a hidden unit at the 
next time step. Another option is to design a RNNs with recurrent 
connections between hidden units that read an entire sequence 
and then produce a single output. Here I will focus on option 
one.


Which option as well as which activation and loss function 
one chooses in practice highly depends on the problem at hand. 
If you are interested in implementing your own first RNN 
I suggest you check out the excellent post of Andrej Karpathy.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/design_pattern1.png) 

(This is the computational graph of design option one 
described above. Here $U, V, W$ are the input-to-hidden, 
hidden-to-output and hidden-to-hidden design matrices 
respectively.)

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/design_pattern1_unfolded.png)

(The unfolded graph.)

## Developing the Forward and Backward Propagation Equations

Our aim is now to derive equations to describe the network 
represented by the graphs above mathematically. 
For this we denote the activation function of the i-th hidden 
unit by $f_{i}$, denote the activation function of the 
vector of all $n$ hidden units by $f = \begin{pmatrix}
f_{1} & \ldots & f_{n}
\end{pmatrix}$. For simplicity, we will assume that all 
hidden units have the same activation function which we, 
with a slight abuse of notation, will also call $f$.

Furthermore we will assume discrete output 
(Now we can view the output $o$ as giving unnormalized 
probabilities of each possible value of the discrete 
variable. As a postprocessing step we could then apply the 
softmax-function to normalize the probabilities.). 
Furthermore $h_{0}$ denotes the initial state and $h_{t}$ 
the state at time-step $t$, $x_{t}$ the input at time 
$t$, $b, c$ are bias-vectors and $U, V, W$ the input-to-hidden, 
hidden-to-output and hidden-to hidden weight matrices 
respectively. 

As a consequence we get the following set of equations for 
time-steps $t = 0,\ldots, \tau$:

$$ h_{t} = f(b+Wh_{t-1}+Ux_{t}) = \phi(h_{t-1}, x_{t}, \theta), \quad \text{where } \theta = (W, U, b) \\
o_{t} = c + Vh_{t} \\
\hat{y_{t}} = softmax(o_{t}) $$

The choice of the loss-function naturally depends on the 
problem at hand. For simplicity, we will consider the 
squared error

$$ L = \sum_{t= 0}^{\tau}L_{t} = \sum_{t= 0}^{\tau} \frac{1}{2} \sum_{k:k ~ output ~ unit} (y^{k}_{t}-\hat{y}^{k}_{t})^{2}, $$

where $ y^k_t $ is the training target for output unit $k$ at time $t$.

To train this network we simply apply the back-propagation 
algorithm to our unrolled graph. This is called back-propagation 
through time (BPTT).
The core-idea of back-propagation is to update the weights 
by going as far as the learning rate suggests in the 
direction of the steepest descent of the loss-function:

$$ \Delta \theta = -\alpha \frac{\partial L}{\partial \theta} , $$
here $\theta$ stands for some parameter of the network.

By doing so the loss is getting a bit smaller in every step.


From this formula we see that we need to calculate 
$\frac{\partial L}{\partial \theta}$:

$$ \frac{\partial L}{\partial \theta} = \sum_{t=0}^{\tau} \frac{\partial L_{t}}{\partial \theta} \\
 = \sum_{t=0}^{\tau} \sum_{k=0}^{t} (\frac{\partial L_{t}}{\partial h_{t}^{T}} \frac{\partial h_{t}}{\partial h_{k}^{T}} \frac{\partial^{+} h_{k}}{\partial \theta}) $$

$\frac{\partial^{+} h_{k}}{\partial \theta}$ means that we 
compute the immediate partial derivative, $h_{k-1}$ is 
taken as a constant with respect to $\theta$.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/rnn/forward_backward.png) 

(Graphical representation of forward and backward pass.)

Since $L_{t}$ is in $\mathbb{R}$, $h_{t}$ is a vector, 
let's say in $\mathbb{R}^{n}$ (if you have $n$ hidden units) 
for all $t$, $\frac{\partial L_{t}}{\partial h_{t}}$ is a 
vector of the form:

$$
\frac{\partial L_{t}}{\partial h_{t}^{T}} = 
\begin{pmatrix}
\frac{\partial L_{t}}{\partial h_{t}^{(1)}} & \ldots & \frac{\partial L_{t}}{\partial h_{t}^{(n)}}
\end{pmatrix},
$$

$\frac{\partial h_{t}}{\partial h_{k}^{T}}$ is a matrix in $\mathbb{R}^{n \times n}$ of the form:

$$
\frac{\partial h_{t}}{\partial h_{k}^{T}} = \frac{\partial h_{t}}{\partial h_{k}} = \begin{pmatrix}
\frac{\partial h_{t}^{(1)}}{\partial h_{k}^{(1)}} & \cdots & \frac{\partial h_{t}^{(1)}}{\partial h_{k}^{(n)}} \\
\vdots & \ddots & \vdots \\
\frac{\partial h_{t}^{(n)}}{\partial h_{k}^{(1)}} & \cdots & \frac{\partial h_{t}^{(n)}}{\partial h_{k}^{(n)}}
\end{pmatrix}
$$

and $\frac{\partial^{+} h_{k}}{\partial \theta}$ is a vector 
in $\mathbb{R}^{n}$ as well:

$$
\frac{\partial^{+} h_{k}}{\partial \theta} = \begin{pmatrix}
\frac{\partial^{+} h_{k}^{(1)}}{\partial \theta} & \cdots & \frac{\partial^{+} h_{k}^{n}}{\partial \theta}
\end{pmatrix}^{T}.
$$

## Problems

At this point we encounter a great problem. The partial 
derivative $\frac{\partial h_{t}}{\partial h_{k}}$ takes 
the form of a matrix product of $t-k$ Jacobians
$$
\frac{\partial h_{t}}{\partial h_{k}} = \prod_{i = k+1}^{t} \frac{\partial h_{i}}{\partial h_{i-1}} 
$$
If the difference $t-k$ takes large values ($k \ll t$) in 
most cases this product either vanishes or explodes in 
the sense that the norm either goes to zero or to infinity.

Intuitively, you can think of it in the same way as of a 
product of numbers which goes to zero if the numbers are 
smaller than one and to infinity if the numbers are larger 
than one. Mathematically tight conditions for when this 
happens for products of matrices are given in the paper 
of Pascanu.

One quite simple case is the one where there exists an 
$\eta \in \mathbb{R}$ such that for the spektral-norm 
of $ \frac{\partial h_{i+1}}{\partial h_{i}} $ and for 
all $i$

$$ \parallel \frac{\partial h_{i+1}}{\partial h_{i}} \parallel \leq \eta < 1. $$

In this case it holds for fixed $k$

$$ \mid \frac{\partial L_{t}}{\partial h_{t}^{T}} \frac{\partial h_{t}}{\partial h_{k}^{T}} \frac{\partial^{+} h_{k}}{\partial \theta} \mid \\
 \leq \parallel \frac{\partial L_{t}}{\partial h_{t}^{T}} \frac{\partial h_{t}}{\partial h_{k}^{T}} \parallel_{2} \parallel \frac{\partial^{+} h_{k}}{\partial \theta} \parallel_{2}\\
 = \parallel \frac{\partial L_{t}}{\partial h_{t}^{T}} ( \prod_{i = k+1}^{t} \frac{\partial h_{i}}{\partial h_{i-1}} ) \parallel_{2} \parallel \frac{\partial^{+} h_{k}}{\partial \theta} \parallel_{2} \\
 \leq  \eta^{t-k} \parallel \frac{\partial L_{t}}{\partial h_{t}} \parallel_{2} \parallel \frac{\partial^{+} h_{k}}{\partial \theta} \parallel_{2}, $$
where $ \parallel\cdot\parallel_{2} $ stands for the Euclidean norm.

Now you can see that this expression goes to zero 
for $t\rightarrow \infty$. This means we have to deal with 
vanishing gradients, we loose information over long time 
intervals or in other words the long-term memory of the 
network is not very good. And in the case of exploding 
gradients the long-term components grow exponentially 
faster as the short-term ones and thus overpower them. 

The exploding gradient case is quite easy to treat, e.g. 
we could use methods like gradient clipping (see deep-learning 
book).
The vanishing gradients one on the other hand is not so simple. 
The most promising remedy proposed in the last years is called 
Long Short-Term Memory (LSTM) and will be the topic of my next blogpost.


#### References
Here are some further materials that might be worth checking out:

- [Ian Goodfellow and Yoshua Bengio and Aaron Courville: Deep Learning Chapter 10, In: MIT Press (2016)](http://www.deeplearningbook.org)
- [Alex Graves: Generating Sequences With Recurrent Neural Networks (2014)](https://arxiv.org/pdf/1308.0850.pdf)
- [Alex Graves: Supervised Sequence Labelling with Recurrent Neural Networks, In: Springer (2012)](http://www.springer.com/de/book/9783642247965)
- [Razvan Pascanu and Tomas Mikolov and Yoshua Bengio: On the difficulty of training Recurrent Neural Networks (2013)](https://arxiv.org/pdf/1211.5063.pdf)
- [Andrej Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks (2015)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)




