---
layout: post
comments: true
title:  "Recurrent Neural Networks - 02 Long Short-Term Memory"
excerpt: "Wiebke GÃ¼nther - This is the second part of a series 
of blogposts about Recurrent Neural Networks (RNNs). 
It will cover the topic of Long Short-Term Memory (LSTM)."
date:   2017-09-15
mathjax: true
---

## Introduction
The idea of Long Short-Term Memory was first proposed 
by Hochreiter and Schmidhuber in 1997 to overcome the 
problem of vanishing gradients which we talked about in 
the last post. The aim was to construct a RNN which could 
learn to store information over long time-intervals. 
They achived their goal by constructing 
_Constant Error Carousels (CEC)_ within the RNN through 
which the error can flow backwards unchanged. 
This enables the network to bridge around 1000 time-steps, 
where other RNN architectures are only able to bridge 
about 5 to 10.


Since then the idea has been worked on quite a bit, 
some changes and extensions were proposed. The LSTM we  
will derive here is the one first considered by Graves 
and Schmidhuber in 2005 and has been the most common one 
in applications. It features input-, output- and forget-gates, 
peephole connections and uses full gradient training. 
I will refer to this design as _Vanilla LSTM_.

## Constant Error Flow
First, we will make our notation a bit more clear and explicit:

Denote by $net_{j}(t)$ the net-input into unit $j$ at 
time-step $t$. By $f_{j}$ we will denote the activation 
function of unit $j$, as in the last post $L = \sum_{t=0}^{T} L_t$ 
is the total loss function and $L_{t}$ is the 
loss at time $t$. 

Our aim is to construct a RNN that can store information 
over long time-intervals. We somehow want to get rid of 
the problem of vanishing gradients when back-propagating. 
We want constant error flow.

To get a better grasp on what exactly and mathematically 
constant error flow means, we start with a simple example.

Consider a recurrent neural network with only one unit $j$ 
and no external signal and no output.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/lstm/recurrent_unit_cec.png)

This RNN can be described by the following formula using 
the notation from above:

$$
net_{j}(t+1) = w_{jj}f_{j}(net_{j}(t)) \quad \text{for } t = 0, \ldots, T-1.
$$

Thus we get the following formula for unit $j$'s error:

$$
\frac{\partial L}{\partial net_{j}(t)} \\
 = \frac{\partial L}{\partial net_{j}(t+1)} \frac{\partial net_{j}(t+1)}{\partial net_{j}(t)} \\
 = \frac{\partial L}{\partial net_{j}(t+1)} w_{jj} f'_{j}(net_{j}(t))
$$

A naive approach to try and ensure constant error flow over 
time is to demand

$$
w_{jj} f'_{j}(net_{j}(t)) = 1.
$$

If we integrate this expression we get

$$
f_{j}(net_{j}(t)) = \frac{net_{j}(t)}{w_{jj}}.
$$

This means the activation function $f_{j}$ has to be linear 
and the unit $j$'s activation has to stay constant:

$$
f_{j}(net_{j}(t+1)) = f_{j}(w_{jj} f_{j}(net_{j}(t))) = f_{j}(net_{j}(t)).
$$

We could ensure this by choosing $f_{j}(x)=x ~ \forall x$ 
and $w_{jj} = 1$.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/lstm/connected_cec.png)

But in most cases our unit $j$ is connected to other units, 
let's call them $i$ and $k$. In this scenario our approach 
shows its downsides. Then the weights $w_{ji}$ and $w_{kj}$ 
connecting the units $i$ and $j$ and $j$ and $k$ 
respectively are recieving conflicting weight update signals 
when training the network. For example the weight $w_{ji}$ 
has to be used to store certain inputs and to ignore others. 
Those are two completely opposing tasks which clearly 
makes learning difficult. Analogue problems arise with 
the weight $w_{kj}$.

But how do we deal with those issues? 

The answer is we construct a memory block which can contain 
multiple memory cells and install gates, namely an input 
and an output gate which protect the contents stored in 
the memory cells and within those cells the error can 
flow back in time undisturbed. They are so called constant 
error carrousels (CEC). Don't worry if this seems confusing 
as of now, we'll walk through it step by step.

## Memory Cells and Gate Units

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/lstm/LSTM_block.png)

(LSTM Memory Block $c_{j}$ with one Cell.)

If you want to see another comprehensive breakdown of all parts 
of a LSTM Memory Block done in a diffrent way I highly 
recommend reading Christopher Olah's post about 
Understanding LSTMs.

Again we start by getting the notation for a memory block 
straight.

Let $c_{j}$ denote the $j$-th memory block and $c_{j}^{\nu}$ 
the $\nu$-th unit of $c_{j}$. Input gate units are indexed 
by $in_{j}$, output gate units are denoted by $out_{j}$. 
Let $g$ and $h$ be some differentiable functions. 
The function $g$ squashes the memory cell input and $h$ 
scales the memory cell output.

Then we can define the input into our memory block $c_{j}$ 
in the following way

$$
net_{c_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{c_{j} u}y^{u}(t-1),
$$

where $y^{j}(t)$ is the activation of the unit $j$ at time $t$ 
and $w_{ji}$ denotes the weight connecting units $i$ and $j$.

To understand this formula we have to imagine our memory 
block as a part of a bigger neural network which consists 
of an input layer, a hidden layer in which our memory block, 
among other hidden units, lives and an output layer. 
The network could look something like that:

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/lstm/LSTM_net.png)

(This image shows a LSTM with two memory blocks with input and output gates. Only the connections of one of the memory blocks are drawn.)

Now we can see that our memory block recieves a weighted 
sum of the activations of the units that are connected to 
it as input. This is also reflected in our formula.

Analogously, we can define the inputs into the input- and 
output gate units:

$$
net_{in_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{in_{j} u}y^{u}(t-1), \\
net_{out_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{out_{j} u}y^{u}(t-1).
$$

And for all other hidden units $i$:

$$
net_{i}(t) = \sum_{u: ~ arbitrary ~ unit}w_{i u}y^{u}(t-1).
$$

For the activations of the input and the output gate at 
time $t$ we get

$$
y^{in_{j}}(t) = f_{in_{j}}(net_{in_{j}}(t)), \\
y^{out_{j}}(t) = f_{out_{j}}(net_{out_{j}}(t)),
$$

as wells for other hidden units $i$:

$$
y^{i}(t) = f_{i}(net_{i}(t)),
$$

where as above $f_{i}$ denotes the activation function of 
unit $i$. 

Again, these equations are like we expected them to be. 
We just apply the activation function to the input of the unit.
But what do these gates do exactly? To answer that question 
we have to take a look at the formula for the internal 
state or cell state for a cell $c_j^{\nu}$ of a memory block $c_j$ itself:

$$
s_{c_{j}^{\nu}}(0) = 0, \qquad s_{c_{j}^{\nu}}(t) = s_{c_{j}^{\nu}}(t-1) + y^{in_{j}}(t) g(net_{c_{j}^{\nu}}(t))
$$

First, we notice that this equation is recursive. And if we 
consider the case where nothing is getting into the cell
($y^{in_{j}}(t) = 0$) then we clearly see that we have 
created some kind of architecture where the error can get 
into and then bridge several time-steps without being 
disturbed: the activation function of the cell is $f(x)=x$ 
and the weights are $w_{s_{c_{j}^{\nu}} s_{c_{j}^{\nu}}} = 1$.

Now we have a look at the second summand in the equation: 
We see that here the activation of the input gate gets 
involved. It somehow decides what gets let into the cell. 
If the activation of the input gate is close to zero almost 
nothing can get in. 

The application of the function $g$ is not that important, 
you can just think of it as some preprocessing step for 
the block input.

The last missing part is the output activation of the 
cell at time $t$:

$$
y^{c_{j}^{\nu}}(t) = y^{out_{j}}(t) h(s_{c_{j}^{\nu}}(t))
$$

We see that the internal state gets scaled by the 
function $h$ and we see the output gate unit at work. 
Analogou to the input gate it decides what and how much of it 
leaves the cell.

## Back-Propagation

We already saw in the last post that we can train RNNs using 
back propagation through time (BPTT). Now we want to derive 
the BPTT-formulas for the LSTM-structure we created above.

Hochreiter and Schmidhuber used truncated BPTT to train their 
LSTM in the first paper published about the topic but Graves 
and Schmidhuber later showed that one could also use full 
gradient BPTT and achive slightly higher performance and 
better verifiability (e.g. through numerical methods).

Remember what BPTT was all about. We update the weights by 
going as far as the learning rate suggests in the direction 
of steepest descent of the loss function:

$$
\Delta w_{l m} = -\alpha \frac{\partial L}{\partial w_{l m}},
$$
where $\alpha$ is the learning rate, $L$ is the loss 
and $w_{l m}$ is the weight between unit $m$ and unit $l$.

We will consider the following loss function

$$
L = \sum_{t= 0}^{\tau}L_{t} = \sum_{t= 0}^{\tau} \frac{1}{2} \sum_{k:k ~ output ~ unit} (y^{k}(t)-t^{k}(t))^{2},
$$

where $t^{k}(t)$ is the training target for output unit $k$ 
at time $t$.

We can write 

$$
\Delta w_{l m}  = -\alpha \frac{\partial L}{\partial w_{l m}} \\
 = - \alpha \sum_{t=0}^{\tau} \frac{\partial L}{\partial net_{l}(t)} \frac{\partial net_{l}(t)}{\partial w_{l m}}.
$$

For $t = 1, \ldots, \tau$ and some units $l$ and $m$ we can compute

$$
\frac{\partial net_{l}(t)}{\partial w_{l m}} = y^{m}(t-1).
$$

This means we get

$$
\Delta w_{l m} = - \alpha \sum_{t=0}^{\tau} \frac{\partial L}{\partial net_{l}(t)} y^{m}(t-1).
$$

Now define some unit $j$s error as

$$
\delta_{j}(t) = \frac{\partial L}{\partial net_{j}(t)}.
$$

With this definition we can write

$$
\Delta w_{l m} = - \alpha \sum_{t=1}^{\tau} \delta_{l}(t) y^{m}(t-1).
$$

We see that the only thing we really have to think about are 
the $\delta$s.
For an output unit $k$ $\delta_{k}$ is quite easy to compute:

$$
\delta_{k}(t) = y^{k}(t)-t^{k}(t).
$$
 
But how do we calculate the $\delta$s for the other parts of 
the network and in particular for the memory block?

For the output gates first notice that only the activation 
of the output gate is a function of the input to the output 
gate and nothing else in the network. Thus we can write

$$
\delta_{out_{j}}(t)  = \frac{\partial L}{\partial net_{out_{j}}(t)} \\
 = \frac{\partial L}{\partial y^{out_{j}}(t)} \frac{\partial y^{out_{j}}(t)}{\partial net_{out_{j}}(t)} \\ 
 = \frac{\partial L}{\partial y^{out_{j}}(t)} f'_{out_{j}}(net_{out_j}(t))\\
$$

For the remaining derivative 
$\frac{\partial L}{\partial y^{out_{j}}(t)}$ notice that the 
activation of the output gate is only used further to 
calculate the cell outputs of its memory block. That's why 
we can split the derivative up in the following way

$$
\frac{\partial L}{\partial y^{out_{j}}(t)}  = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial y^{c_{j}^{\nu}}(t)} \frac{\partial y^{c_{j}^{\nu}}(t)}{\partial y^{out_{j}}(t)}\\
  = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial y^{c_{j}^{\nu}}(t)} h(s_{c_{j}^{\nu}}(t))
$$

Now it remains to calculate the loss of the output of 
cell $c_{j}^{\nu}$. This cell output can be connected to all 
kinds of units. Since we will be needing this again we name 
it $\epsilon_{c_{j}^{\nu}}$.

$$
\epsilon_{c_{j}^{\nu}}(t) := \frac{\partial L}{\partial y^{c_{j}^{\nu}}(t)} 
 = \sum_{\text{units } u \text{ that are connected to }c_{j}^{\nu}} \frac{\partial L}{\partial net_{u}(t+1)} \frac{\partial net_{u}(t+1)}{\partial y^{c_{j} ^{\nu}}(t)}\\
 = \sum_{\text{units } u \text{ that are connected to }c_{j}^{\nu}} \frac{\partial L}{\partial net_{u}(t+1)} w_{u c_{j}^{\nu}} \\
 = \sum_{\text{units } u \text{ that are connected to }c_{j}^{\nu}} \delta_{u} (t+1) w_{u c_{j}^{\nu}}
$$

Ok, so what do we get altogether for the error at the output 
gates?

$$
\delta_{out_{j}}(t) = f'_{out_{j}}(net_{out_j}(t)) \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} h(s_{c_{j}^{\nu}}(t)) \epsilon_{c_{j}^{\nu}}(t)
$$

From this formula we see that the error arriving at the output 
gate gets scaled by the output non-linearity $h$.

Now the error enters the memory cells. We expect it to only 
be scaled again when leaving the cell but staying constant 
while being inside the cell. Is this really what happens? 

For all cells $c_{j}^{\nu}$ in every memory block $c_{j}$ we 
find that the internal state is used by the output gate, 
the input gate at the next time-step, the internal state at 
the next time step and the cell output. Thus we can split 
up $\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)}$ in the 
following way

$$
\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)}  = \frac{\partial L}{\partial net_{out_{j}}(t)} \frac{\partial net_{out_{j}}(t)}{\partial s_{c_{j}^{\nu}}(t)} + \frac{\partial L}{\partial net_{in_{j}}(t+1)} \frac{\partial net_{in_{j}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} + \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} \frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} + \frac{\partial L}{\partial y^{c_{j}^{\nu}}(t)} \frac{\partial y^{c_{j}^{\nu}}(t)}{\partial s_{c_{j}^{\nu}}(t)}\\
 = \delta_{out_{j}}(t) w_{out_j c_j^{\nu}} + \delta_{in_{j}}(t+1) w_{in_j c_j^{\nu}} + \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} \frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} + \epsilon_{c_{j}^{\nu}}(t) y^{out_{j}}(t) h'(s_{c_{j}^{\nu}})\\
$$

The most important part of this equation is the 
summand $\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} \frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)}$. 
It's the one that ensures that the gradient can't completely 
vanish. Even if all the other parts of the equation above 
would tend to zero, this one would ensure that the error 
can bridge several time steps. This happens since

$$
\frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} = 1.
$$

So if all the other contributions to the gradient vanish 
from time step $t$ to time step $t+k$ we still get something 
like that

$$
\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} \approx \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+k)}. 
$$

But of course the error can leave the cell again through 
the input gate. So, for the error at the block input at 
time $t$ we get

$$
\delta_{c_{j}}(t)  = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} \frac{\partial s_{c_{j}^{\nu}}(t)}{\partial net_{c_{j}}(t)} \\
 = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} y^{in_{j}}(t)g'(net_{c_j^{\nu}}(t)).
$$

The last part of the memory block we need to consider is the 
input gate. Its input is only used to calculate its activation. 
Thus we can write

$$
\delta_{in_{j}}(t)  = \frac{\partial L}{\partial y^{in_j}(t)} \frac{\partial y^{in_j}(t)}{\partial net_{in_{j}}(t)} \\
 = \frac{\partial L}{\partial y^{in_j}(t)} f'_{in_j}(net_{in_j}(t)).
$$

The activation of the input gate is then used to calculate 
the states of the cells of its block. Thus

$$
\frac{\partial L}{\partial y^{in_j}(t)}  = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} \frac{\partial s_{c_{j}^{\nu}}(t)}{\partial y^{in_j}(t)} \\
 = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} g(net_{c_j^{\nu}}(t)).
$$

This formula shows us that the error gets scaled once again 
when leaving the cells through the input gate.

Now we have derived all formulas that we need to update our 
weights. Great, that means we are done, we have achieved 
our aim to construct a neural network which can operate on 
sequences and can learn context over long time intervals. 
But what happens if we try to use our network for processing 
a continual input stream?

The cells will recieve more and more information to store. 
The state will continue to grow and to grow untill our 
network isn't able to function anymore. The problem is: 
Our LSTM is not able to forget anything. To prevent this 
breakdown from happening we would have to reset the state 
manually to zero. This means the network would only able to 
process data that is a priori segmented with marked 
beginning/ ending points where we could reset the state.

The solution to this problem is called _Forget Gate_. 
Adding forget gates is the next step on our way to the 
Vanilla LSTM.

## Forget Gates - Learning to Forget

Forget Gates were first introduced by Gers, Schmidhuber and 
Cummins in 1999. They wanted to extend the existing LSTM 
structure in such a way that the network could learn to forget 
(which means reset its internal state) at appropriate times.

This process of forgetting doesn't have to happen abruptly, 
the internal state doesn't have to be reset to zero immediately, 
but it also could happen gradually. The cell state could be 
fading out, so to speak.

So, how can one achieve all of this?

The idea is to replace the constant CEC weight 
($w_{s_{c_{j}} s_{c_{j}}} = 1$) by the multiplicative forget 
gate activation $y^{\phi_j}$. This is shown in the following 
image.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/lstm/LSTM_forget.png)

(LSTM Memory Block $c_{j}$ with a Forget Gate.)

How does that translate into our forward and backward pass 
formulas?

### Forward Pass

The good news is that it doesn't change the formulas we derived 
above that much. The only one that really is getting changed 
is the one for the internal state as you would have guessed 
from looking at the graphical representation. And of course 
we have to add formulas for the forget gate's input and 
activation.

First, have a look at the input of the forget gate unit at 
time $t$:

$$
net_{\phi_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{\phi_{j} u}y^{u}(t-1),
$$
where $y^{u}(t)$ is again the activation of unit $u$ at time $t$ 
and $w_{\phi_{j} u}$ is the weight on the connection between 
unit $u$ and the forget gate.

And its activation:

$$
y^{\phi_{j}}(t) = f_{\phi_{j}}(net_{\phi_{j}}(t)),
$$
where $f_{\phi_{j}}$ is the activation function of the 
forget gate.

Now, what about the formula of the internal state?

Remember the equation for the internal state of a LSTM memory 
block  without a forget gate, notation as above:

$$
s_{c_{j}^{\nu}}(0) = 0, \qquad s_{c_{j}^{\nu}}(t) = s_{c_{j}^{\nu}}(t-1) + y^{in_{j}}(t) g(net_{c_{j}^{\nu}}(t)).
$$

Now we have to replace it with

$$
s_{c_{j}^{\nu}}(0) = 0, \qquad s_{c_{j}^{\nu}}(t) = y^{\phi_j}(t) s_{c_{j}^{\nu}}(t-1) + y^{in_{j}}(t) g(net_{c_{j}^{\nu}}(t)).
$$

We see that we replaced the constant weight of the self-recurrent 
connection of the internal state by the forget gate activation.

### Backward Pass
The training of a LSTM with forget gates can also be done
by BPTT. Since in the forward pass only the formula for 
the internal state has changed and the ones for the forget 
gate were added, we can imagine that also the formulas for 
the backward pass we derived above don't change dramatically. 
The general procedure is the same. We want to do a weight 
update through

$$
\Delta w_{l m} = -\alpha \frac{\partial L}{\partial w_{l m}} \\
 = - \alpha \sum_{t=1}^{T} \delta_{j}(t) y^{m}(t-1),
$$

where $\delta_{j}(t) := \frac{\partial L}{\partial net_{j}(t)}$
for some unit $j$ as above and we use the same loss 
function $L$.

Thus we again just need to worry about the $\delta$s. They are 
calculated in the same way as above for the output gates. 
The ones for the block input and input gates change since the 
internal state's error 
$\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)}$ is calculated 
differently. Since the internal state is now also used in 
computing the input for the forget gate in the next time step:

$$
\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} 
 = \delta_{out_{j}}(t) w_{out_j c_j^{\nu}} + \delta_{in_{j}}(t+1) w_{in_j c_j^{\nu}} \\ + \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} \frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} + \epsilon_{c_{j}^{\nu}}(t) y^{out_{j}}(t) h'(s_{c_{j}^{\nu}}) + \frac{\partial L}{\partial net_{\phi_{j}}(t+1)}\frac{\partial net_{\phi_{j}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} \\
 = \delta_{out_{j}}(t) w_{out_j c_j^{\nu}} + \delta_{in_{j}}(t+1) w_{in_j c_j^{\nu}} \\ + \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} \frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} + \epsilon_{c_{j}^{\nu}}(t) y^{out_{j}}(t) h'(s_{c_{j}^{\nu}}) + \delta_{\phi_j}(t+1)w_{\phi_j c_j^{\nu}}
$$

But now we have 

$$
\frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} \frac{\partial s_{c_{j}^{\nu}}(t+1)}{\partial s_{c_{j}^{\nu}}(t)} = \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t+1)} y^{\phi}(t+1).
$$

This means the recursion depends on the actual activation of the 
blocks forget gate. Which in turn implies that not only the cell's 
state is reset when the activation goes to zero but the partial 
derivatives are also reset. We can interpret this as forgiving 
previous mistakes.

And, as you would have expected, we have to add a formula for 
the forget gate unit's error at time $t$. Its input is only used 
to calculate its activation. Thus we can write

$$
\delta_{\phi_{j}}(t) = \frac{\partial L}{\partial y^{\phi_{j}}(t)} \frac{\partial y^{\phi_j}(t)}{\partial net_{\phi_{j}}(t)} \\
 = \frac{\partial L}{\partial y^{\phi_j}(t)} f'_{\phi_j}(net_{\phi_j}(t)).
$$

The activation of the forget gate is then used to calculate the 
states of the cells of its block. Thus

$$
\frac{\partial L}{\partial y^{\phi_j}(t)} = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} \frac{\partial s_{c_{j}^{\nu}}(t)}{\partial y^{in_j}(t)} \\
 = \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} g(net_{c_j}(t)).
$$

Together we get the following formula for the forget gate's error

$$
\delta_{\phi_{j}}(t) = f'_{\phi_j}(net_{\phi_j}(t)) \sum_{c_{j}^{\nu} \text{ is cell in block } c_j} \frac{\partial L}{\partial s_{c_{j}^{\nu}}(t)} g(net_{c_j^{\nu}}(t)).
$$


## Peephole Connections - Learning to Time and Count

So far we have constructed a neural network that is able to learn 
contextual relations over long time intervals and can solve 
sequentiell processing tasks with continual input.
Now we want to increase its abilities to learn to use information 
that is contained in the size of the time intervals between events. 
For example, this is important in rhythm detection.

Gers and Schmidhuber tried to solve this problem in 2000 and 
their idea was to add what they called _Peephole Connections_.

They thought that maybe the LSTM is limited by the fact that each 
gate only recieves information from the input units and the 
outputs of all cells. This way the gate units have no direct 
connection to the CEC which they have to control.

So they added weighted connections from the CEC to the gates of 
the same memory block. During the learning phase these connections 
are inactive. That means no error signals are getting propagated 
back along those peephole connections. That way the shield of the 
gates is still intact. At all other times they are treated as 
regular connections.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/lstm/LSTM_peep.png)

Since with the peephole connections we added a second kind of 
recurrent connections to the LSTM we now need to be careful in 
what order we go through the forward pass and update the parts of 
the Vanilla LSTM.
The updates happen in two phases and in case there are recurrent 
connections from the gates phase one is further divided into three 
steps. The notation is the same as above.

1. 1. Input gate activation $y^{in}$
$$
net_{in_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{in_{j} u}y^{u}(t-1) + \sum_{c_j^{\nu}: ~ cell ~ of ~ memory ~ block ~ c_j} w_{in_{j} c_j^{\nu}} s_{c_j^{\nu}}(t-1), \\
y^{in_j}(t) = f_{in_j}(net_{in_j}(t))
$$ 
     2. Forget gate activation $y^{\phi}$
$$
net_{\phi_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{\phi_{j} u}y^{u}(t-1) + \sum_{c_j^{\nu}: ~ cell ~ of ~ memory ~ block ~ c_j} w_{\phi_{j} c_j^{\nu}} s_{c_j^{\nu}}(t-1), \\
y^{\phi_j}(t) = f_{\phi_j}(net_{\phi_j}(t))
$$
    3. Cell input and cell state $s_{c}$
$$
net_{c_j^{\nu}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{c_j^{\nu} u}y^{u}(t-1), \\
s_{c_j^{\nu}}(t) = y^{\phi_j}(t) s_{c_j^{\nu}}(t-1) + y^{in_j}(t) g(net_{c_j^{\nu}}(t)), \quad s_{c_j^{\nu}}(0) = 0
$$

2. Output gate activation $y^{out}$ and cell output $y^{c}$
$$
net_{out_{j}}(t) = \sum_{u: ~ arbitrary ~ unit}w_{out_{j} u}y^{u}(t-1) + \sum_{c_j^{\nu}: ~ cell ~ of ~ memory ~ block ~ c_j} w_{out_{j} c_j^{\nu}} y^{out_j}(t) s_{c_j^{\nu}}(t), \\
y^{out_j}(t) = f_{out_j}(net_{out_j}(t))
$$

We won't go over the backward pass again since there are only 
minor changes to what we've seen before.

## Final Remarks

Now we have, in all detail, derived the Vanilla LSTM structure as described in the introduction. With this structure or some slight variations of it people were able to achive some quite impressive things: They have been used for handwriting recognition and generation, for language modeling and translation and for accoustic modeling of speech to only name a few. So it is safe to say that the development of the LSTM architecture has been a big step for the usefulness of RNNs. But is that all? Or can we develop the idea of recurrent neural networks even farther to achieve even more?
There sure are some very exciting ideas out there that are worth exploring. My next post will focus on Neural Turing Machines which are a recurrent structure that combines a controller network with a external memory bank.

#### References

Here are some further materials that might be worth checking out:

- [Ian Goodfellow and Yoshua Bengio and Aaron Courville: Deep Learning, In: MIT Press (2016)](http://www.deeplearningbook.org)
- [Sepp Hochreiter and JÃ¼rgen Schmidhuber: Long Short-Term Memory (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf)
- [Colah's blog: Understanding LSTM Networks (2015)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Klaus Greff et al: LSTM: A Search Space Odyssey (2015)](https://arxiv.org/pdf/1503.04069.pdf#cite.Gers1999@-1)
- [Felix A. Gers and JÃ¼rgen Schmidhuber and Fred Cummins: Learning to Forget: Continual Prediction with LSTM (1999)](https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf)
- [Felix A. Gers and JÃ¼rgen Schmidhuber: Recurrent Nets that Time and Count (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)
- [Alex Graves and JÃ¼rgen Schmidhuber: Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures (2005)](ftp://ftp.idsia.ch/pub/juergen/nn_2005.pdf)
