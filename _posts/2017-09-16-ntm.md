---
layout: post
comments: true
title:  "Recurrent Neural Networks - 03 neural Turing Machine"
excerpt: "Wiebke GÃ¼nther - This is the third part of a series 
of blogposts about Recurrent Neural Networks (RNNs). 
It will cover the topic of Neural Turing Machines (NTM)."
date:   2017-09-14
mathjax: true
---

## Introduction
There were a lot of great things achieved using recurrent neural networks, most of them using LSTMs, 
which we discussed in the last post. LSTMs could be trained to recognize and generate handwriting, 
text and music.

But all of the above are tasks involving more intuitive thinking when we do them as humans. Other tasks 
like sorting or copying a list of numbers, finding the shortest path when navigating that were done by 
computers ages ago are very hard to learn for feedforward neural networks as well as LSTMs.
So we somehow need to find a way to merge the neural network approach with a classical computer.

In a brain analogy one could say those neural network architectures lack a working memory and thus 
aren't able to learn to solve those tasks. So we need to add such a memory.

Whether you like the computer- or neuroscience point of view best: Neural Turing Machines can be seen 
as a solution in both cases.

So first, let's have a look at a classical Turing Machine and then figure out how to make it "neural".

## Classical Turing Machine

A Turing machine is composed of a reading and a writing head that operate on a memory tape which is 
divided in discrete cells. It is a simple mathematical model which is able to simulate any computer 
algorithm's logic when manipulating symbols on the tape according to a table of rules.

![f]()
\includegraphics[scale=0.3]{classic_turing.png} 

## Making it Neural

The Neural Turing Machine (NTM) works in a very similar way. The biggest change is that the controller is 
now a neural network.

If you want to look at it from a biological point of view you could say that we have added a working memory 
to a neural network.

![f]()
\includegraphics[scale=0.4]{NTM_overview.png}  

Since we want to train the NTM by gradient descent and, in particular, want it to learn where to read from 
and where to write to in the memory tape, we have to make sure that the whole structure is differentiable. 
This means that it has to be differentiable with respect to the location where it reads from or writes to. 
Here lays another big difference to a classical Turing machine or digital computer. The reading and writing 
heads of a classical Turing machine only adress a single discrete element at once. Thus their reading and 
writing operations are not differentiable.

Instead, in the NTM we have to make those operations blurry. This means the NTM reads and writes 
everywhere but to different extends. One could say it focuses its attention on different parts of the 
memory bank. This focus is formalized in the attention distribution which we will discuss later.

## The Read and Write Heads

First, let's have a look at how the NTM reads from the memory bank.

Let $M_t$ be the contents of the $M \times N$ memory matrix at time $t$. Here $N$ is the number of memory 
locations and $M$ is the size of the vector stored at each location. 

Let $a_t$ be the vector that stores the attention distribution for this read-operation at time $t$. 
(It has to hold $\sum_{i=1}^{N}w_t(i) = 1$ and $w_t(i) \geq 0$ for $i = 1,\ldots, N$, where $a_t(i)$ stands 
for the i-th entry of $a_t$.) This means the vector $w_t$ contains how much we care about the memory 
positions.

Then the reading head returns the vector $r_t$ which is of length $M$ and is defined by

$$
r_t = \sum_{i=1}^{N} w_t(i) M_t(i),
$$

where $M_t(i)$ stands for the i-th row of $M_t$.
This vector is clearly differentiable with respect to the weightings $w_t(i)$ as well as to the memory.

![f]()
\includegraphics[scale=0.5]{read.png} 

To describe the write-operation we use the same notation as above, but now $w_t$ stands for the attention 
vector of the writing head at time $t$. The write-operation gives us a new memory matrix. This is done 
in two steps: erase followed by add (this is analogous to the LSTM memory cell where getting new input 
and forgetting were also done separately). Each write head also produces an erase vector $e_t$, whose 
elements lie in $(0,1)$, and an add vector $a_t$ at each time step $t$.

1. The erase-step

$$
\hat{M}_{t}(i) = M_{t-1}(i) - w_t(i) M_{t-1}(i) \cdot e_t,
$$

where the $[\cdot$]-operation acts point-wise. This means the elements of a memory location are only 
reset to zero if both $w_t(i)$ and $e_t(i)$ are one. If one of both is zero the memory isn't changed.

![f]()
\includegraphics[scale=0.5]{erase(1).png} 

2. The add-step

$$
M_{t}(i) = \hat{M}_{t}(i) + w_t(i) a_t
$$

![f]()
\includegraphics[scale=0.5]{add.png} 

Both erase and add are differentiable and thus the combined write operation is, too.

## The Attention Distribution
The question that is still unanswered is: How do we get the attention distribution? Or in other words: 
How do we decide where to focus our attention?

This is done by using a combination of two approaches: _Content-based_ attention and 
_location-based_ attention. The following chart shows how the attention distribution is calculated 
step by step. I also recommend to have a look at the original paper of Graves on Neural Turing machines 
since the steps are described there in a very clear way. 

The first part of this calculation is content-based, the attention is focused on the parts of the memory 
whose current values are similar to the values produced by the controller.

Unfortunately, not all problems are well-suited for this kind of adressing. Problems arise for example 
when the content of a variable is arbitrary but it still needs a name or adress, which is the case for 
example with arithmetic problems. Here we need to use the location-based approach. This happens in the 
second part of the diagram below.

![f]()
\includegraphics[scale=0.5]{attention_distr.png} 
