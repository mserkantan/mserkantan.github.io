---
layout: post
comments: true
title:  "Recurrent Neural Networks - 03 Neural Turing Machine"
excerpt: "Wiebke GÃ¼nther - This is the third part of a series 
of blogposts about Recurrent Neural Networks (RNNs). 
It will cover the topic of Neural Turing Machines (NTM)."
date:   2017-09-16
mathjax: true
---

## Introduction
There were a lot of great things achieved using recurrent neural networks, most of them using LSTMs, 
which we discussed in the last post. LSTMs could be trained to recognize and generate handwriting, 
text and music.

But all of the above are tasks involving more intuitive thinking when we do them as humans. Other tasks 
like sorting or copying a list of numbers, finding the shortest path when navigating that were done by 
computers ages ago are very hard to learn for feedforward neural networks as well as LSTMs.
So we somehow need to find a way to merge the neural network approach with a classical computer.

In a brain analogy one could say those neural network architectures lack a working memory and thus 
aren't able to learn to solve those tasks. So we need to add such a memory.

Whether you like the computer- or neuroscience point of view best: Neural Turing Machines can be seen 
as a solution in both cases.

So first, let's have a look at a classical Turing Machine and then figure out how to make it "neural".

## Classical Turing Machine

A Turing machine is composed of a reading and a writing head that operate on a memory tape which is 
divided in discrete cells. It is a simple mathematical model which is able to simulate any computer 
algorithm's logic when manipulating symbols on the tape according to a table of rules.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/classic_turing.png)

## Making it Neural

The Neural Turing Machine (NTM) works in a very similar way. The biggest change is that the controller is 
now a neural network.

If you want to look at it from a biological point of view you could say that we have added a working memory 
to a neural network.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/NTM_overview.png) 

Since we want to train the NTM by gradient descent and, in particular, want it to learn where to read from 
and where to write to in the memory tape, we have to make sure that the whole structure is differentiable. 
This means that it has to be differentiable with respect to the location where it reads from or writes to. 
Here lays another big difference to a classical Turing machine or digital computer. The reading and writing 
heads of a classical Turing machine only adress a single discrete element at once. Thus their reading and 
writing operations are not differentiable.

Instead, in the NTM we have to make those operations blurry. This means the NTM reads and writes 
everywhere but to different extends. One could say it focuses its attention on different parts of the 
memory bank. This focus is formalized in the attention distribution which we will discuss later.

## The Read and Write Heads

First, let's have a look at how the NTM writes to the memory bank.

Let $M_t$ be the contents of the $M \times N$ memory matrix at time $t$. Here $N$ is the number of memory 
locations and $M$ is the size of the vector stored at each location. 

Let $w_t$ be the vector that stores the attention distribution for 
this write-operation at time $t$. 
(It has to hold $\sum_{i=1}^{N}w_t(i) = 1$ and $w_t(i) \geq 0$ 
for $i = 1,\ldots, N$, where $w_t(i)$ stands 
for the i-th entry of $w_t$.) This means the vector $w_t$ contains how much we care about the memory 
positions.

The write-operation gives us a new memory matrix. This is done in two steps: 
erase followed by add (this is analogous to the LSTM memory cell where getting new input 
and forgetting were also done separately). Each write head also produces an erase vector $e_t$, whose 
elements lie in $(0,1)$, and an add vector $a_t$ at each time step $t$.

1. The erase-step

$$
\hat{M}_{t}(i) = M_{t-1}(i) - w_t(i) M_{t-1}(i) \cdot e_t, \qquad (1)
$$

where the $[\cdot$]-operation acts point-wise. This means the elements of a memory location are only 
reset to zero if both $w_t(i)$ and $e_t(i)$ are one. If one of both is zero the memory isn't changed.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/erase(1).png)

2. The add-step

$$
M_{t}(i) = \hat{M}_{t}(i) + w_t(i) a_t \qquad (2)
$$

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/add.png)

Both erase and add are differentiable and thus the combined write operation is, too.

To describe the read-operation we use the same notation as above, 
but now $w_t$ stands for the attention vector of the reading head 
at time $t$.

The reading head returns the vector $r_t$ which is of length $M$ 
and is defined by

$$
r_t = \sum_{i=1}^{N} w_t(i) M_t(i), \qquad (3)
$$

where $M_t(i)$ stands for the i-th row of $M_t$.
This vector is clearly differentiable with respect to the weightings $w_t(i)$ as well as to the memory.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/read.png)


## The Attention Distribution
The question that is still unanswered is: How do we get the attention distribution? Or in other words: 
How do we decide where to focus our attention?

This is done by using a combination of two approaches: _Content-based_ attention and 
_location-based_ attention. The following chart shows how the attention distribution is calculated 
step by step. I also recommend to have a look at the original paper of Graves on Neural Turing machines 
since the steps are described there in a very clear way. 

The first part of this calculation is content-based, the attention is focused on the parts of the memory 
whose current values are similar to the values produced by the controller.

Unfortunately, not all problems are well-suited for this kind of adressing. Problems arise for example 
when the content of a variable is arbitrary but it still needs a name or adress, which is the case for 
example with arithmetic problems. Here we need to use the location-based approach. This happens in the 
second part of the diagram below.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/attention_distr.png)

## Bringing it all together

To make things a bit clearer let's have a look at what exactly happens at time $t$:

For notational convenience we assume that we only have one read and 
one write head.
At each time step, the controller network receives an external input 
vector $x_t$ and the read vector $r_{t-1}$ which was obtained from 
the memory matrix at the previous time step.
Furthermore, the controller network emits an output vector $y_t$ and 
an interface vector $\xi_t \in \mathbb{R}^{M+(2S+1)+3+3M+(2S+1)+3}$,
where $M$ is the size of the vectors stored in the memory matrix and $S$ is the shift range. 
The interface vector allows the neural network to interact with the 
memory matrix.

After that the interface vector gets split up in the following way:

$$
\xi_t = (k_t^{r}, \hat{\beta}_t^r, \hat{g}_t^r, \hat{s}_t^r, \hat{\gamma}_t^r, k_t^{w}, \hat{e}_t, a_t, \hat{\beta}_t^w, \hat{g}_t^w, s_t^w, \hat{\gamma}_t^w)
$$,

where $k_t^{r} \in \mathbb{R}^M$ is the read key, 
$\hat{\beta}_t^r \in \mathbb{R}$, $\hat{g}_t^r \in \mathbb{R}$, $\hat{s}_t^r \in \mathbb{R}^(2S+1)$ and
$\hat{\gamma}_t^r \in \mathbb{R}$ and $k_t^{w} \in \mathbb{R}^M$ is the 
write key, $\hat{e}_t \in \mathbb{R}^M$, $a_t \in \mathbb{R}^M$
is the add vector, $\hat{\beta}_t^w \in \mathbb{R}$, 
$\hat{g}_t^w \in \mathbb{R}$, $\hat{s}_t^w \in \mathbb{R}^(2S+1)$ and
$\hat{\gamma}_t^w \in \mathbb{R}$.

The individual components are processed to ensure that they lie in the correct
domain:

- the read key $k_t^{r} \in \mathbb{R}^M$
- the read strength $\beta_t^r = 1+log(1+exp(\hat{\beta}_t^r)) \in [1,\infty)$
- the read interpolation gate $g_t^r = \sigma(\hat{g}_t^r) \in [0,1]$
- the read shift weighting $s_t^r = softmax(\hat{s}_t^r)$
- the read sharpening scalar $\gamma_t^r = 1+log(1+exp(\hat{\gamma}_t^r) \in [1, \infty)$
- the write key $k_t^{w} \in \mathbb{R}^M$
- the erase vector $e_t = \sigma(\hat{e}_t) \in [0,1]^M$
- the add vector $a_t \in \mathbb{R}^M$
- the write strength $\beta_t^w = 1+log(1+exp(\hat{\beta}_t^w)) \in [1,\infty)$
- the write interpolation gate $g_t^w = \sigma(\hat{g}_t^w) \in [0,1]$
- the write shift weighting $s_t^w = softmax(\hat{s}_t^w)$
- the write sharpening scalar $\gamma_t^w = 1+log(1+exp(\hat{\gamma}_t^w)) \in [1, \infty)$

Then they are used to compute the attention distributions for the
write head ($w_t^w$). This is done using the memory matrix $M_{t-1}$ 
from the previous time step as shown in the image above.
Afterwards, $w_t^w$, the memory matrix $M_{t-1}$, the erase 
vector $e_t$ and the add vector $a_t$  are used 
to compute the new memory matrix $M_t$ as in equations (1) and (2).

This new memory matrix $M_{t}$ is then utilized to calculated 
the attention distribution $w_t^r$ for the read head as shown in 
the image. After that the read vector $r_t$ is computed according 
to equation (3).

The full algorithm is shown in the following piece of pseudocode.

![f](https://raw.githubusercontent.com/neurocats/neurocats.github.io/master/assets/ntm/pseudocode_ntm.png)

## Final Remarks

The design of the Neural Turing Machine explores a quite interesting and promising research direction in the field of neural networks: memory and attention. The NTM is able to learn simple algorithms from sample data.

Despite this remarkable achievement the NTM still has some faults and obvious drawbacks:

It has no way of ensuring that blocks of memory that already have been assigned with information do not overlap and interfere. Furthermore, it is not able to forget things that it has written to its memory and thus can't reuse memory when processing long sequences. The NTM also has problems when trying to preserve the sequential order of information. The only way in which it can achieve this is by writing in consecutive locations.

So it seems obvious that the Neural Turing Machine needs some kind of smarter mechanism of interacting with its memory. The architecture that addresses these problems is called Differentiable Neural Computer (DNC). More information on this topic can be found [here](https://deepmind.com/blog/differentiable-neural-computers/).

Another big problem one has to face when implementing a NTM is that they are quite hard to train due to their large unfolded depth. On top of that they do not scale since they access their complete memory in a linear way.

These issues have since been addressed by using alternative memory architectures such as in the [Neural GPU](https://arxiv.org/pdf/1511.08228.pdf) or in [this paper](https://arxiv.org/pdf/1510.03931.pdf) which proposes several different structures of memory for NTM.

#### References

Here are some further materials that might be worth checking out:

- [Christopher Olah and Shan Carter: Attention and Augmented Recurrent Neural Networks (2016)](https://distill.pub/2016/augmented-rnns/)
- [Alex Graves and Greg Wayne and Ivo Danihelka: Neural Turing Machines (2014)](https://arxiv.org/pdf/1410.5401.pdf)
- [Ian Goodfellow and Yoshua Bengio and Aaron Courville: Deep Learning Chapter 10.12, In: MIT Press (2016)](http://www.deeplearningbook.org)
